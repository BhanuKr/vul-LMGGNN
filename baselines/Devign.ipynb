{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ffe01e-f23e-4c0c-88f1-63a427100f22",
   "metadata": {},
   "source": [
    "# Devign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6401e8a-d20b-43cc-a603-9d6ae284851f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a43cc-b9b8-4c22-a6a2-6c02064a078d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff148fe7-d086-4509-adef-13c2640875d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, config, file_path=\"dataset/Devign/configs.json\"):\n",
    "        with open(file_path) as config_file:\n",
    "            self._config = json.load(config_file)\n",
    "            self._config = self._config.get(config)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_property(self, property_name):\n",
    "        return self._config.get(property_name)\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.device\n",
    "\n",
    "    def all(self):\n",
    "        return self._config\n",
    "\n",
    "\n",
    "class Create(Config):\n",
    "    def __init__(self):\n",
    "        super().__init__('create')\n",
    "\n",
    "    @property\n",
    "    def filter_column_value(self):\n",
    "        return self.get_property('filter_project')\n",
    "\n",
    "    @property\n",
    "    def slice_size(self):\n",
    "        return self.get_property('slice_size')\n",
    "\n",
    "    @property\n",
    "    def joern_cli_dir(self):\n",
    "        return self.get_property('joern_cli_dir')\n",
    "\n",
    "\n",
    "class Data(Config):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    @property\n",
    "    def cpg(self):\n",
    "        return self.get_property('cpg')\n",
    "\n",
    "    @property\n",
    "    def raw(self):\n",
    "        return self.get_property('raw')\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self.get_property('input')\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.get_property('model')\n",
    "\n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return self.get_property('tokens')\n",
    "\n",
    "    @property\n",
    "    def w2v(self):\n",
    "        return self.get_property('w2v')\n",
    "\n",
    "\n",
    "class Paths(Data):\n",
    "    def __init__(self):\n",
    "        super().__init__('paths')\n",
    "\n",
    "    @property\n",
    "    def joern(self):\n",
    "        return self.get_property('joern')\n",
    "\n",
    "\n",
    "class Files(Data):\n",
    "    def __init__(self):\n",
    "        super().__init__('files')\n",
    "\n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return self.get_property('tokens')\n",
    "\n",
    "    @property\n",
    "    def w2v(self):\n",
    "        return self.get_property('w2v')\n",
    "\n",
    "\n",
    "class Embed(Config):\n",
    "    def __init__(self):\n",
    "        super().__init__('embed')\n",
    "\n",
    "    @property\n",
    "    def nodes_dim(self):\n",
    "        return self.get_property('nodes_dim')\n",
    "\n",
    "    @property\n",
    "    def w2v_args(self):\n",
    "        return self.get_property('word2vec_args')\n",
    "\n",
    "    @property\n",
    "    def edge_type(self):\n",
    "        return self.get_property('edge_type')\n",
    "\n",
    "\n",
    "class Process(Config):\n",
    "    def __init__(self):\n",
    "        super().__init__('process')\n",
    "\n",
    "    @property\n",
    "    def epochs(self):\n",
    "        return self.get_property('epochs')\n",
    "\n",
    "    @property\n",
    "    def patience(self):\n",
    "        return self.get_property('patience')\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.get_property('batch_size')\n",
    "\n",
    "    @property\n",
    "    def dataset_ratio(self):\n",
    "        return self.get_property('dataset_ratio')\n",
    "\n",
    "    @property\n",
    "    def shuffle(self):\n",
    "        return self.get_property('shuffle')\n",
    "\n",
    "\n",
    "class Devign_class(Config):\n",
    "    def __init__(self):\n",
    "        super().__init__('devign')\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.get_property('learning_rate')\n",
    "\n",
    "    @property\n",
    "    def weight_decay(self):\n",
    "        return self.get_property('weight_decay')\n",
    "\n",
    "    @property\n",
    "    def loss_lambda(self):\n",
    "        return self.get_property('loss_lambda')\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.get_property('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7ee0bf-c1a2-494e-b454-88a8c7140f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATHS = Paths()\n",
    "FILES = Files()\n",
    "DEVICE = FILES.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc55af06-c020-45a4-9a0c-daa5c5add559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "\n",
    "# Clean Gadget\n",
    "# Author https://github.com/johnb110/VDPython:\n",
    "# For each gadget, replaces all user variables with \"VAR#\" and user functions with \"FUN#\"\n",
    "# Removes content from string and character literals keywords up to C11 and C++17; immutable set\n",
    "from typing import List\n",
    "\n",
    "keywords = frozenset({'__asm', '__builtin', '__cdecl', '__declspec', '__except', '__export', '__far16', '__far32',\n",
    "                    '__fastcall', '__finally', '__import', '__inline', '__int16', '__int32', '__int64', '__int8',\n",
    "                    '__leave', '__optlink', '__packed', '__pascal', '__stdcall', '__system', '__thread', '__try',\n",
    "                    '__unaligned', '_asm', '_Builtin', '_Cdecl', '_declspec', '_except', '_Export', '_Far16',\n",
    "                    '_Far32', '_Fastcall', '_finally', '_Import', '_inline', '_int16', '_int32', '_int64',\n",
    "                    '_int8', '_leave', '_Optlink', '_Packed', '_Pascal', '_stdcall', '_System', '_try', 'alignas',\n",
    "                    'alignof', 'and', 'and_eq', 'asm', 'auto', 'bitand', 'bitor', 'bool', 'break', 'case',\n",
    "                    'catch', 'char', 'char16_t', 'char32_t', 'class', 'compl', 'const', 'const_cast', 'constexpr',\n",
    "                    'continue', 'decltype', 'default', 'delete', 'do', 'double', 'dynamic_cast', 'else', 'enum',\n",
    "                    'explicit', 'export', 'extern', 'false', 'final', 'float', 'for', 'friend', 'goto', 'if',\n",
    "                    'inline', 'int', 'long', 'mutable', 'namespace', 'new', 'noexcept', 'not', 'not_eq', 'nullptr',\n",
    "                    'operator', 'or', 'or_eq', 'override', 'private', 'protected', 'public', 'register',\n",
    "                    'reinterpret_cast', 'return', 'short', 'signed', 'sizeof', 'static', 'static_assert',\n",
    "                    'static_cast', 'struct', 'switch', 'template', 'this', 'thread_local', 'throw', 'true', 'try',\n",
    "                    'typedef', 'typeid', 'typename', 'union', 'unsigned', 'using', 'virtual', 'void', 'volatile',\n",
    "                    'wchar_t', 'while', 'xor', 'xor_eq', 'NULL'})\n",
    "# holds known non-user-defined functions; immutable set\n",
    "main_set = frozenset({'main'})\n",
    "# arguments in main function; immutable set\n",
    "main_args = frozenset({'argc', 'argv'})\n",
    "\n",
    "operators3 = {'<<=', '>>='}\n",
    "operators2 = {\n",
    "    '->', '++', '--', '**',\n",
    "    '!~', '<<', '>>', '<=', '>=',\n",
    "    '==', '!=', '&&', '||', '+=',\n",
    "    '-=', '*=', '/=', '%=', '&=', '^=', '|='\n",
    "}\n",
    "operators1 = {\n",
    "    '(', ')', '[', ']', '.',\n",
    "    '+', '&',\n",
    "    '%', '<', '>', '^', '|',\n",
    "    '=', ',', '?', ':',\n",
    "    '{', '}', '!', '~'\n",
    "}\n",
    "\n",
    "def to_regex(lst):\n",
    "    return r'|'.join([f\"({re.escape(el)})\" for el in lst])\n",
    "\n",
    "\n",
    "regex_split_operators = to_regex(operators3) + to_regex(operators2) + to_regex(operators1)\n",
    "\n",
    "\n",
    "# input is a list of string lines\n",
    "def clean_gadget(gadget):\n",
    "    # dictionary; map function name to symbol name + number\n",
    "    fun_symbols = {}\n",
    "    # dictionary; map variable name to symbol name + number\n",
    "    var_symbols = {}\n",
    "\n",
    "    fun_count = 1\n",
    "    var_count = 1\n",
    "\n",
    "    # regular expression to find function name candidates\n",
    "    rx_fun = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?=\\s*\\()')\n",
    "    # regular expression to find variable name candidates\n",
    "    # rx_var = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?!\\s*\\()')\n",
    "    rx_var = re.compile(r'\\b([_A-Za-z]\\w*)\\b((?!\\s*\\**\\w+))(?!\\s*\\()')\n",
    "\n",
    "    # final cleaned gadget output to return to interface\n",
    "    cleaned_gadget = []\n",
    "\n",
    "    for line in gadget:\n",
    "        # replace any non-ASCII characters with empty string\n",
    "        ascii_line = re.sub(r'[^\\x00-\\x7f]', r'', line)\n",
    "        # remove all hexadecimal literals\n",
    "        hex_line = re.sub(r'0[xX][0-9a-fA-F]+', \"HEX\", ascii_line)\n",
    "        # return, in order, all regex matches at string list; preserves order for semantics\n",
    "        user_fun = rx_fun.findall(hex_line)\n",
    "        user_var = rx_var.findall(hex_line)\n",
    "\n",
    "        # Could easily make a \"clean gadget\" type class to prevent duplicate functionality\n",
    "        # of creating/comparing symbol names for functions and variables in much the same way.\n",
    "        # The comparison frozenset, symbol dictionaries, and counters would be class scope.\n",
    "        # So would only need to pass a string list and a string literal for symbol names to\n",
    "        # another function.\n",
    "        for fun_name in user_fun:\n",
    "            if len({fun_name}.difference(main_set)) != 0 and len({fun_name}.difference(keywords)) != 0:\n",
    "                # check to see if function name already in dictionary\n",
    "                if fun_name not in fun_symbols.keys():\n",
    "                    fun_symbols[fun_name] = 'FUN' + str(fun_count)\n",
    "                    fun_count += 1\n",
    "                # ensure that only function name gets replaced (no variable name with same\n",
    "                # identifier); uses positive lookforward\n",
    "                hex_line = re.sub(r'\\b(' + fun_name + r')\\b(?=\\s*\\()', fun_symbols[fun_name], hex_line)\n",
    "\n",
    "        for var_name in user_var:\n",
    "            # next line is the nuanced difference between fun_name and var_name\n",
    "            if len({var_name[0]}.difference(keywords)) != 0 and len({var_name[0]}.difference(main_args)) != 0:\n",
    "                # check to see if variable name already in dictionary\n",
    "                if var_name[0] not in var_symbols.keys():\n",
    "                    var_symbols[var_name[0]] = 'VAR' + str(var_count)\n",
    "                    var_count += 1\n",
    "                # ensure that only variable name gets replaced (no function name with same\n",
    "                # identifier); uses negative lookforward\n",
    "                # print(var_name, gadget, user_var)\n",
    "                hex_line = re.sub(r'\\b(' + var_name[0] + r')\\b(?:(?=\\s*\\w+\\()|(?!\\s*\\w+))(?!\\s*\\()',\n",
    "                                  var_symbols[var_name[0]], hex_line)\n",
    "\n",
    "        cleaned_gadget.append(hex_line)\n",
    "    # return the list of cleaned lines\n",
    "    return cleaned_gadget\n",
    "\n",
    "\n",
    "# Cleaner & Tokenizer\n",
    "# Author https://github.com/hazimhanif/svd-transformer/blob/master/transformer_svd.ipynb\n",
    "\n",
    "def tokenizer(code, flag=False):\n",
    "    gadget: List[str] = []\n",
    "    tokenized: List[str] = []\n",
    "    # remove all string literals\n",
    "    no_str_lit_line = re.sub(r'[\"]([^\"\\\\\\n]|\\\\.|\\\\\\n)*[\"]', '', code)\n",
    "    # remove all character literals\n",
    "    no_char_lit_line = re.sub(r\"'.*?'\", \"\", no_str_lit_line)\n",
    "    code = no_char_lit_line\n",
    "\n",
    "    if flag:\n",
    "        code = codecs.getdecoder(\"unicode_escape\")(no_char_lit_line)[0]\n",
    "\n",
    "    for line in code.splitlines():\n",
    "        if line == '':\n",
    "            continue\n",
    "        stripped = line.strip()\n",
    "        # if \"\\\\n\\\\n\" in stripped: print(stripped)\n",
    "        gadget.append(stripped)\n",
    "\n",
    "    clean = clean_gadget(gadget)\n",
    "\n",
    "    for cg in clean:\n",
    "        if cg == '':\n",
    "            continue\n",
    "\n",
    "        # Remove code comments\n",
    "        pat = re.compile(r'(/\\*([^*]|(\\*+[^*\\/]))*\\*+\\/)|(\\/\\/.*)')\n",
    "        cg = re.sub(pat, '', cg)\n",
    "\n",
    "        # Remove newlines & tabs\n",
    "        cg = re.sub('(\\n)|(\\\\\\\\n)|(\\\\\\\\)|(\\\\t)|(\\\\r)', '', cg)\n",
    "        # Mix split (characters and words)\n",
    "        splitter = r' +|' + regex_split_operators + r'|(\\/)|(\\;)|(\\-)|(\\*)'\n",
    "        cg = re.split(splitter, cg)\n",
    "\n",
    "        # Remove None type\n",
    "        cg = list(filter(None, cg))\n",
    "        cg = list(filter(str.strip, cg))\n",
    "        # code = \" \".join(code)\n",
    "        # Return list of tokens\n",
    "        tokenized.extend(cg)\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d73be95-f756-4d22-856c-ce3455fdff78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.5.0-py3-none-any.whl.metadata (64 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.24.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.5)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch_geometric)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch_geometric)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
      "Using cached torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 torch_geometric-2.5.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02a0918-6c9b-46e8-bb37-f4c628f26760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "# from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class InputDataset(TorchDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return self.dataset.iloc[index].input\n",
    "        data = self.dataset.iloc[index].input\n",
    "        data.func = self.dataset.iloc[index].func  # 添加现有的func属性, 为 codebert 的输入准备\n",
    "        return data\n",
    "\n",
    "    def get_loader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(dataset=self, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdaeebef-2928-4393-98a2-bcc6e5a64ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read(path, json_file):\n",
    "  \"\"\"\n",
    "  :param path: str\n",
    "  :param json_file: str\n",
    "  :return DataFrame\n",
    "  \"\"\"\n",
    "  return pd.read_json(path + json_file)\n",
    "\n",
    "\n",
    "def get_ratio(dataset, ratio):\n",
    "    approx_size = int(len(dataset) * ratio)\n",
    "    return dataset[:approx_size]\n",
    "\n",
    "\n",
    "def load(path, pickle_file, ratio=1):\n",
    "    dataset = pd.read_pickle(path + pickle_file)\n",
    "    dataset.info(memory_usage='deep')\n",
    "    if ratio < 1:\n",
    "        dataset = get_ratio(dataset, ratio)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def write(data_frame: pd.DataFrame, path, file_name):\n",
    "    data_frame.to_pickle(path + file_name)\n",
    "\n",
    "\n",
    "def apply_filter(data_frame: pd.DataFrame, filter_func):\n",
    "    return filter_func(data_frame)\n",
    "\n",
    "\n",
    "def rename(data_frame: pd.DataFrame, old, new):\n",
    "    return data_frame.rename(columns={old: new})\n",
    "\n",
    "\n",
    "def tokenize(data_frame: pd.DataFrame):\n",
    "    data_frame[\"tokens\"] = data_frame[\"func\"].apply(tokenizer)\n",
    "    # Change column name\n",
    "    # data_frame.rename(columns={\"func\": \"tokens\"}, inplace=True)\n",
    "    # Keep just the tokens\n",
    "    return data_frame[[\"tokens\", \"func\"]]\n",
    "\n",
    "def to_files(data_frame: pd.DataFrame, out_path):\n",
    "    # path = f\"{self.out_path}/{self.dataset_name}/\"\n",
    "    if os.path.exists(out_path):\n",
    "        return\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "    for idx, row in data_frame.iterrows():\n",
    "        file_name = f\"{idx}.c\"\n",
    "        with open(out_path + file_name, 'w') as f:\n",
    "            f.write(row.func)\n",
    "\n",
    "\n",
    "def create_with_index(data, columns):\n",
    "    data_frame = pd.DataFrame(data, columns=columns)\n",
    "    data_frame.index = list(data_frame[\"Index\"])\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def inner_join_by_index(df1, df2):\n",
    "    return pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "def train_val_test_split(data_frame: pd.DataFrame, shuffle=True):\n",
    "    print(\"Splitting Dataset\")\n",
    "\n",
    "    false = data_frame[data_frame.target == 0]\n",
    "    true = data_frame[data_frame.target == 1]\n",
    "\n",
    "    train_false, test_false = train_test_split(false, test_size=0.2, shuffle=shuffle)\n",
    "    test_false, val_false = train_test_split(test_false, test_size=0.5, shuffle=shuffle)\n",
    "    train_true, test_true = train_test_split(true, test_size=0.2, shuffle=shuffle)\n",
    "    test_true, val_true = train_test_split(test_true, test_size=0.5, shuffle=shuffle)\n",
    "\n",
    "    # train = train_false.append(train_true)\n",
    "    train = pd.concat([train_false, train_true])\n",
    "\n",
    "    # val = val_false.append(val_true)\n",
    "    val = pd.concat([val_false, val_true])\n",
    "\n",
    "    # test = test_false.append(test_true)\n",
    "    test =pd.concat([test_false, test_true])\n",
    "\n",
    "    train = train.reset_index(drop=True)\n",
    "    val = val.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "\n",
    "    return InputDataset(train), InputDataset(test), InputDataset(val)\n",
    "\n",
    "\n",
    "def get_directory_files(directory):\n",
    "    return [os.path.basename(file) for file in glob.glob(f\"{directory}/*.pkl\")]\n",
    "\n",
    "\n",
    "def loads(data_sets_dir, ratio=1):\n",
    "    data_sets_files = sorted([f for f in listdir(data_sets_dir) if isfile(join(data_sets_dir, f))])\n",
    "\n",
    "    if ratio < 1:\n",
    "        data_sets_files = get_ratio(data_sets_files, ratio)\n",
    "\n",
    "    dataset = load(data_sets_dir, data_sets_files[0])\n",
    "    data_sets_files.remove(data_sets_files[0])\n",
    "\n",
    "    for ds_file in data_sets_files:\n",
    "        #dataset = dataset.append(load(data_sets_dir, ds_file))\n",
    "        # 使用 pd.concat 来连接两个 DataFrame\n",
    "        dataset = pd.concat([dataset, load(data_sets_dir, ds_file)])\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def clean(data_frame: pd.DataFrame):\n",
    "    return data_frame.drop_duplicates(subset=\"func\", keep=False)\n",
    "\n",
    "\n",
    "def drop(data_frame: pd.DataFrame, keys):\n",
    "    for key in keys:\n",
    "        del data_frame[key]\n",
    "\n",
    "\n",
    "def slice_frame(data_frame: pd.DataFrame, size: int):\n",
    "    data_frame_size = len(data_frame)\n",
    "    return data_frame.groupby(np.arange(data_frame_size) // size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "445b132a-a332-40ef-91ca-0fa436d8bf71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_indexing(graph):\n",
    "    idx = int(graph[\"file\"].split(\".c\")[0].split(\"/\")[-1])\n",
    "    del graph[\"file\"]\n",
    "    return idx, {\"functions\": [graph]}\n",
    "\n",
    "def joern_parse(joern_path, input_path, output_path, file_name):\n",
    "    out_file = file_name + \".bin\"\n",
    "\n",
    "    # 使用 os.path.join 构建绝对路径\n",
    "    joern_parse_cmd = [os.path.join(joern_path, \"joern-parse\"), input_path, \"--out\",\n",
    "                       os.path.join(output_path, out_file)]\n",
    "\n",
    "    try:\n",
    "        # 使用 check=True 处理异常\n",
    "        joern_parse_call = subprocess.run(joern_parse_cmd, stdout=subprocess.PIPE, text=True, check=True)\n",
    "        print(str(joern_parse_call))\n",
    "        return out_file\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while running joern-parse: {e}\")\n",
    "        # 可以根据需要执行其他错误处理操作\n",
    "        return None\n",
    "\n",
    "\n",
    "def joern_create(joern_path, in_path, out_path, cpg_files):\n",
    "    joern_process = subprocess.Popen([\"./\" + joern_path + \"joern\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    json_files = []\n",
    "    for cpg_file in cpg_files:\n",
    "        json_file_name = f\"{cpg_file.split('.')[0]}.json\"\n",
    "        json_files.append(json_file_name)\n",
    "\n",
    "        print(in_path+cpg_file)\n",
    "        if os.path.exists(in_path+cpg_file):\n",
    "            json_out = f\"{os.path.abspath(out_path)}/{json_file_name}\"\n",
    "            import_cpg_cmd = f\"importCpg(\\\"{os.path.abspath(in_path)}/{cpg_file}\\\")\\r\".encode()\n",
    "            script_path = f\"{os.path.dirname(os.path.abspath(joern_path))}/graph-for-funcs.sc\"\n",
    "            run_script_cmd = f\"cpg.runScript(\\\"{script_path}\\\").toString() |> \\\"{json_out}\\\"\\r\".encode()\n",
    "            joern_process.stdin.write(import_cpg_cmd)\n",
    "            print(joern_process.stdout.readline().decode())\n",
    "            joern_process.stdin.write(run_script_cmd)\n",
    "            print(joern_process.stdout.readline().decode())\n",
    "            joern_process.stdin.write(\"delete\\r\".encode())\n",
    "            print(joern_process.stdout.readline().decode())\n",
    "    try:\n",
    "        outs, errs = joern_process.communicate(timeout=60)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        joern_process.kill()\n",
    "        outs, errs = joern_process.communicate()\n",
    "    if outs is not None:\n",
    "        print(f\"Outs: {outs.decode()}\")\n",
    "    if errs is not None:\n",
    "        print(f\"Errs: {errs.decode()}\")\n",
    "    return json_files\n",
    "\n",
    "\n",
    "def json_process(in_path, json_file):\n",
    "    if os.path.exists(in_path+json_file):\n",
    "        with open(in_path+json_file) as jf:\n",
    "            cpg_string = jf.read()\n",
    "            cpg_string = re.sub(r\"io\\.shiftleft\\.codepropertygraph\\.generated\\.\", '', cpg_string)\n",
    "            cpg_json = json.loads(cpg_string)\n",
    "            container = [graph_indexing(graph) for graph in cpg_json[\"functions\"] if graph[\"file\"] != \"N/A\"]\n",
    "            return container\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97cd59fa-12ce-48be-85a5-aec9aef797d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging.config\n",
    "\n",
    "FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n",
    "\n",
    "logging.basicConfig(filename=\"logs.log\",\n",
    "                    filemode='a',\n",
    "                    format=FORMAT,\n",
    "                    datefmt='%d/%m/%Y %I:%M:%S')\n",
    "\n",
    "\n",
    "def log_info(logger_name, msg):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(msg)\n",
    "\n",
    "def log_warning(logger_name, msg):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.WARNING)\n",
    "    logger.warning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba6c005-aaed-4648-ab1a-60c9da1af076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.1\n",
      "  Using cached gensim-3.8.1-cp310-cp310-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1) (1.11.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1) (7.0.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim==3.8.1) (1.14.1)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.2\n",
      "    Uninstalling gensim-4.3.2:\n",
      "      Successfully uninstalled gensim-4.3.2\n",
      "Successfully installed gensim-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe375bad-178b-4bef-81f3-467f85ced75e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Using cached gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.8.1\n",
      "    Uninstalling gensim-3.8.1:\n",
      "      Successfully uninstalled gensim-3.8.1\n",
      "Successfully installed gensim-4.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d3fa8bc-10e7-4659-972d-675193022ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class NodesEmbedding:\n",
    "    def __init__(self, nodes_dim: int, w2v_keyed_vectors: Word2VecKeyedVectors):\n",
    "        self.w2v_keyed_vectors = w2v_keyed_vectors\n",
    "        self.kv_size = w2v_keyed_vectors.vector_size\n",
    "        self.nodes_dim = nodes_dim\n",
    "\n",
    "        assert self.nodes_dim >= 0\n",
    "\n",
    "        # Buffer for embeddings with padding\n",
    "        self.target = torch.zeros(self.nodes_dim, self.kv_size + 1).float()\n",
    "\n",
    "    def __call__(self, nodes):\n",
    "        embedded_nodes = self.embed_nodes(nodes)\n",
    "        nodes_tensor = torch.from_numpy(embedded_nodes).float()\n",
    "\n",
    "        self.target[:nodes_tensor.size(0), :] = nodes_tensor\n",
    "\n",
    "        return self.target\n",
    "\n",
    "    def embed_nodes(self, nodes):\n",
    "        embeddings = []\n",
    "\n",
    "        for n_id, node in nodes.items():\n",
    "            # Get node's code\n",
    "            node_code = node.get_code()\n",
    "            # Tokenize the code\n",
    "            tokenized_code = tokenizer(node_code, True)\n",
    "            if not tokenized_code:\n",
    "                # print(f\"Dropped node {node}: tokenized code is empty.\")\n",
    "                msg = f\"Empty TOKENIZED from node CODE {node_code}\"\n",
    "                log_warning('embeddings', msg)\n",
    "                continue\n",
    "            # Get each token's learned embedding vector\n",
    "            vectorized_code = np.array(self.get_vectors(tokenized_code, node))\n",
    "            # The node's source embedding is the average of it's embedded tokens\n",
    "            source_embedding = np.mean(vectorized_code, 0)\n",
    "            # The node representation is the concatenation of label and source embeddings\n",
    "            embedding = np.concatenate((np.array([node.type]), source_embedding), axis=0)\n",
    "            embeddings.append(embedding)\n",
    "        # print(node.label, node.properties.properties.get(\"METHOD_FULL_NAME\"))\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    # fromTokenToVectors\n",
    "    def get_vectors(self, tokenized_code, node):\n",
    "        vectors = []\n",
    "\n",
    "        for token in tokenized_code:\n",
    "            if token in self.w2v_keyed_vectors.vocab:\n",
    "                vectors.append(self.w2v_keyed_vectors[token])\n",
    "            else:\n",
    "                # print(node.label, token, node.get_code(), tokenized_code)\n",
    "                vectors.append(np.zeros(self.kv_size))\n",
    "                if node.label not in [\"Identifier\", \"Literal\", \"MethodParameterIn\", \"MethodParameterOut\"]:\n",
    "                    msg = f\"No vector for TOKEN {token} in {node.get_code()}.\"\n",
    "                    log_warning('embeddings', msg)\n",
    "\n",
    "        return vectors\n",
    "\n",
    "\n",
    "class GraphsEmbedding:\n",
    "    def __init__(self, edge_type):\n",
    "        self.edge_type = edge_type\n",
    "\n",
    "    def __call__(self, nodes):\n",
    "        connections = self.nodes_connectivity(nodes)\n",
    "\n",
    "        return torch.tensor(connections).long()\n",
    "\n",
    "    # nodesToGraphConnectivity\n",
    "    def nodes_connectivity(self, nodes):\n",
    "        # nodes are ordered by line and column\n",
    "        coo = [[], []]\n",
    "\n",
    "        for node_idx, (node_id, node) in enumerate(nodes.items()):\n",
    "            if node_idx != node.order:\n",
    "                raise Exception(\"Something wrong with the order\")\n",
    "\n",
    "            for e_id, edge in node.edges.items():\n",
    "                if edge.type != self.edge_type:\n",
    "                    continue\n",
    "\n",
    "                if edge.node_in in nodes and edge.node_in != node_id:\n",
    "                    coo[0].append(nodes[edge.node_in].order)\n",
    "                    coo[1].append(node_idx)\n",
    "\n",
    "                if edge.node_out in nodes and edge.node_out != node_id:\n",
    "                    coo[0].append(node_idx)\n",
    "                    coo[1].append(nodes[edge.node_out].order)\n",
    "\n",
    "        return coo\n",
    "\n",
    "def nodes_to_input(nodes, target, nodes_dim, keyed_vectors, edge_type):\n",
    "    nodes_embedding = NodesEmbedding(nodes_dim, keyed_vectors)\n",
    "    graphs_embedding = GraphsEmbedding(edge_type)\n",
    "    label = torch.tensor([target]).float()\n",
    "\n",
    "    return Data(x=nodes_embedding(nodes), edge_index=graphs_embedding(nodes), y=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5724eed0-0a20-4dd4-8aaa-97ce01ed24ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Properties:\n",
    "\tdef __init__(self, props, indentation):\n",
    "\t\tself.size = len(props)\n",
    "\t\tself.indentation = indentation + 1\n",
    "\t\tself.pairs = {prop[\"key\"]: prop[\"value\"] for prop in props}\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\tindentation = self.indentation * \"\\t\"\n",
    "\t\tstring = \"\"\n",
    "\n",
    "\t\tfor prop in self.pairs:\n",
    "\t\t\tstring += f\"\\n{indentation}Property - {prop} : {self.pairs[prop]}\"\n",
    "\n",
    "\t\treturn f\"{indentation}{string}\\n\"\n",
    "\n",
    "\tdef code(self):\n",
    "\t\tif self.has_code():\n",
    "\t\t\tcode = self.pairs[\"CODE\"]\n",
    "\t\t\tif self.has_type() and self.get_type() != \"ANY\" and self.get_type() not in code:\n",
    "\t\t\t\tcode = f\"{self.get_type()} {code}\"\n",
    "\t\t\treturn code\n",
    "\t\treturn None\n",
    "\n",
    "\tdef get_type(self):\n",
    "\t\treturn self.pairs.get(\"TYPE_FULL_NAME\")\n",
    "\n",
    "\tdef has_type(self):\n",
    "\t\treturn \"TYPE_FULL_NAME\" in self.pairs\n",
    "\n",
    "\tdef has_code(self):\n",
    "\t\treturn \"CODE\" in self.pairs\n",
    "\n",
    "\tdef line_number(self):\n",
    "\t\treturn self.pairs[\"LINE_NUMBER\"] if self.has_line_number() else None\n",
    "\n",
    "\tdef has_line_number(self):\n",
    "\t\treturn \"LINE_NUMBER\" in self.pairs\n",
    "\n",
    "\tdef column_number(self):\n",
    "\t\treturn self.pairs[\"COLUMN_NUMBER\"] if self.has_column_number() else None\n",
    "\n",
    "\tdef has_column_number(self):\n",
    "\t\treturn \"COLUMN_NUMBER\" in self.pairs\n",
    "\n",
    "\tdef get(self):\n",
    "\t\treturn self.pairs\n",
    "\n",
    "\tdef get_operator(self):\n",
    "\t\tvalue = self.pairs.get(\"METHOD_FULL_NAME\")\n",
    "\t\tif value is None:\n",
    "\t\t\treturn value\n",
    "\t\tif (\"<operator>\" in value) or (\"<operators>\" in value):\n",
    "\t\t\treturn value.split(\".\")[-1]\n",
    "\t\treturn None\n",
    "\n",
    "class Edge:\n",
    "\tdef __init__(self, edge, indentation):\n",
    "\t\tself.id = edge[\"id\"].split(\".\")[-1]\n",
    "\t\tself.type = self.id.split(\"@\")[0]\n",
    "\t\tself.node_in = edge[\"in\"].split(\".\")[-1]\n",
    "\t\tself.node_out = edge[\"out\"].split(\".\")[-1]\n",
    "\t\tself.indentation = indentation + 1\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\tindentation = self.indentation * \"\\t\"\n",
    "\t\treturn f\"\\n{indentation}Edge id: {self.id}\\n{indentation}Node in: {self.node_in}\\n{indentation}Node out: {self.node_out}\\n\"\n",
    "\n",
    "\n",
    "node_labels = [\"Block\", \"Call\", \"Comment\", \"ControlStructure\", \"File\", \"Identifier\", \"FieldIdentifier\", \"Literal\",\n",
    "               \"Local\", \"Member\", \"MetaData\", \"Method\", \"MethodInst\", \"MethodParameterIn\", \"MethodParameterOut\",\n",
    "               \"MethodReturn\", \"Namespace\", \"NamespaceBlock\", \"Return\", \"Type\", \"TypeDecl\", \"Unknown\"]\n",
    "\n",
    "operators = ['addition', 'addressOf', 'and', 'arithmeticShiftRight', 'assignment',\n",
    "             'assignmentAnd', 'assignmentArithmeticShiftRight', 'assignmentDivision',\n",
    "             'assignmentMinus', 'assignmentMultiplication', 'assignmentOr', 'assignmentPlus',\n",
    "             'assignmentShiftLeft', 'assignmentXor', 'cast', 'conditionalExpression',\n",
    "             'division', 'equals', 'fieldAccess', 'greaterEqualsThan', 'greaterThan',\n",
    "             'indirectFieldAccess', 'indirectIndexAccess', 'indirection', 'lessEqualsThan',\n",
    "             'lessThan', 'logicalAnd', 'logicalNot', 'logicalOr', 'minus', 'modulo', 'multiplication',\n",
    "             'not', 'notEquals', 'or', 'postDecrement', 'plus', 'postIncrement', 'preDecrement',\n",
    "             'preIncrement', 'shiftLeft', 'sizeOf', 'subtraction']\n",
    "\n",
    "node_labels += operators\n",
    "\n",
    "node_labels = {label: i for i, label in enumerate(node_labels)}\n",
    "\n",
    "PRINT_PROPS = True\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, node, indentation):\n",
    "        self.id = node[\"id\"].split(\".\")[-1]\n",
    "        self.label = self.id.split(\"@\")[0]\n",
    "        self.indentation = indentation + 1\n",
    "        self.properties = Properties(node[\"properties\"], self.indentation)\n",
    "        self.edges = {edge[\"id\"].split(\".\")[-1]: Edge(edge, self.indentation) for edge in node[\"edges\"]}\n",
    "        self.order = None\n",
    "        operator = self.properties.get_operator()\n",
    "        self.label = operator if operator is not None else self.label\n",
    "        self._set_type()\n",
    "\n",
    "    def __str__(self):\n",
    "        indentation = self.indentation * \"\\t\"\n",
    "        properties = f\"{indentation}Properties: {self.properties}\\n\"\n",
    "        edges_str = \"\"\n",
    "\n",
    "        for edge in self.edges:\n",
    "            edges_str += f\"{self.edges[edge]}\"\n",
    "\n",
    "        return f\"\\n{indentation}Node id: {self.id}\\n{properties if PRINT_PROPS else ''}{indentation}Edges: {edges_str}\"\n",
    "\n",
    "    def connections(self, connections, e_type):\n",
    "        for e_id, edge in self.edges.items():\n",
    "            if edge.type != e_type: continue\n",
    "\n",
    "            if edge.node_in in connections[\"in\"] and edge.node_in != self.id:\n",
    "                connections[\"in\"][self.id] = edge.node_in\n",
    "\n",
    "            if edge.node_out in connections[\"out\"] and edge.node_out != self.id:\n",
    "                connections[\"out\"][self.id] = edge.node_out\n",
    "\n",
    "        return connections\n",
    "\n",
    "    def has_code(self):\n",
    "        return self.properties.has_code()\n",
    "\n",
    "    def has_line_number(self):\n",
    "        return self.properties.has_line_number()\n",
    "\n",
    "    def get_code(self):\n",
    "        return self.properties.code()\n",
    "\n",
    "    def get_line_number(self):\n",
    "        return self.properties.line_number()\n",
    "\n",
    "    def get_column_number(self):\n",
    "        return self.properties.column_number()\n",
    "\n",
    "    def _set_type(self):\n",
    "        # label = self.label if self.operator is None else self.operator\n",
    "        self.type = node_labels.get(self.label)  # Label embedding\n",
    "\n",
    "        if self.type is None:\n",
    "            log_warning(\"node\", f\"LABEL {self.label} not in labels!\")\n",
    "            self.type = len(node_labels) + 1\n",
    "\n",
    "class AST:\n",
    "    def __init__(self, nodes, indentation):\n",
    "        self.size = len(nodes)\n",
    "        self.indentation = indentation + 1\n",
    "        self.nodes = {node[\"id\"].split(\".\")[-1]: Node(node, self.indentation) for node in nodes}\n",
    "\n",
    "    def __str__(self):\n",
    "        indentation = self.indentation * \"\\t\"\n",
    "        nodes_str = \"\"\n",
    "\n",
    "        for node in self.nodes:\n",
    "            nodes_str += f\"{indentation}{self.nodes[node]}\"\n",
    "\n",
    "        return f\"\\n{indentation}Size: {self.size}\\n{indentation}Nodes:{nodes_str}\"\n",
    "\n",
    "    def get_nodes_type(self):\n",
    "        return {n_id: node.type for n_id, node in self.nodes.items()}\n",
    "\n",
    "class Function:\n",
    "    def __init__(self, function):\n",
    "        self.name = function[\"function\"]\n",
    "        self.id = function[\"id\"].split(\".\")[-1]\n",
    "        self.indentation = 1\n",
    "        self.ast = AST(function[\"AST\"], self.indentation)\n",
    "\n",
    "    def __str__(self):\n",
    "        indentation = self.indentation * \"\\t\"\n",
    "        return f\"{indentation}Function Name: {self.name}\\n{indentation}Id: {self.id}\\n{indentation}AST:{self.ast}\"\n",
    "\n",
    "    def get_nodes(self):\n",
    "        return self.ast.nodes\n",
    "\n",
    "    def get_nodes_types(self):\n",
    "        return self.ast.get_nodes_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ead7a89d-7ec2-4e35-8d56-e0f73e4b6eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def order_nodes(nodes, max_nodes):\n",
    "    # sorts nodes by line and column\n",
    "\n",
    "    nodes_by_column = sorted(nodes.items(), key=lambda n: n[1].get_column_number())\n",
    "    nodes_by_line = sorted(nodes_by_column, key=lambda n: n[1].get_line_number())\n",
    "\n",
    "    for i, node in enumerate(nodes_by_line):\n",
    "        node[1].order = i\n",
    "\n",
    "    if len(nodes) > max_nodes:\n",
    "        print(f\"CPG cut - original nodes: {len(nodes)} to max: {max_nodes}\")\n",
    "        return OrderedDict(nodes_by_line[:max_nodes])\n",
    "\n",
    "    return OrderedDict(nodes_by_line)\n",
    "\n",
    "\n",
    "def filter_nodes(nodes):\n",
    "    return {n_id: node for n_id, node in nodes.items() if node.has_code() and\n",
    "            node.has_line_number() and\n",
    "            node.label not in [\"Comment\", \"Unknown\"]}\n",
    "\n",
    "def parse_to_nodes(cpg, max_nodes=500):\n",
    "    nodes = {}\n",
    "    for function in cpg[\"functions\"]:\n",
    "        func = Function(function)\n",
    "        # Only nodes with code and line number are selected\n",
    "        filtered_nodes = filter_nodes(func.get_nodes())\n",
    "        nodes.update(filtered_nodes)\n",
    "\n",
    "    return order_nodes(nodes, max_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7e3e188-c8e7-4b5b-8f52-e5d408ac2672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import GatedGraphConv\n",
    "import torch\n",
    "torch.manual_seed(2020)\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "def get_conv_mp_out_size(in_size, last_layer, mps):\n",
    "    size = in_size\n",
    "\n",
    "    for mp in mps:\n",
    "        size = round((size - mp[\"kernel_size\"]) / mp[\"stride\"] + 1)\n",
    "\n",
    "    size = size + 1 if size % 2 != 0 else size\n",
    "\n",
    "    return int(size * last_layer[\"out_channels\"])\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, conv1d_1, conv1d_2, maxpool1d_1, maxpool1d_2, fc_1_size, fc_2_size):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1d_1_args = conv1d_1\n",
    "        self.conv1d_1 = nn.Conv1d(**conv1d_1)\n",
    "        self.conv1d_2 = nn.Conv1d(**conv1d_2)\n",
    "\n",
    "        fc1_size = get_conv_mp_out_size(fc_1_size, conv1d_2, [maxpool1d_1, maxpool1d_2])\n",
    "        fc2_size = get_conv_mp_out_size(fc_2_size, conv1d_2, [maxpool1d_1, maxpool1d_2])\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(fc1_size, 1)\n",
    "        self.fc2 = nn.Linear(fc2_size, 1)\n",
    "\n",
    "        # Dropout\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.mp_1 = nn.MaxPool1d(**maxpool1d_1)\n",
    "        self.mp_2 = nn.MaxPool1d(**maxpool1d_2)\n",
    "\n",
    "    def forward(self, hidden, x):\n",
    "        concat = torch.cat([hidden, x], 1)\n",
    "        concat_size = hidden.shape[1] + x.shape[1]\n",
    "        concat = concat.view(-1, self.conv1d_1_args[\"in_channels\"], concat_size)\n",
    "\n",
    "        Z = self.mp_1(F.relu(self.conv1d_1(concat)))\n",
    "        Z = self.mp_2(self.conv1d_2(Z))\n",
    "\n",
    "        hidden = hidden.view(-1, self.conv1d_1_args[\"in_channels\"], hidden.shape[1])\n",
    "\n",
    "        Y = self.mp_1(F.relu(self.conv1d_1(hidden)))\n",
    "        Y = self.mp_2(self.conv1d_2(Y))\n",
    "\n",
    "        Z_flatten_size = int(Z.shape[1] * Z.shape[-1])\n",
    "        Y_flatten_size = int(Y.shape[1] * Y.shape[-1])\n",
    "\n",
    "        Z = Z.view(-1, Z_flatten_size)\n",
    "        Y = Y.view(-1, Y_flatten_size)\n",
    "        res = self.fc1(Z) * self.fc2(Y)\n",
    "        res = self.drop(res)\n",
    "        # res = res.mean(1)\n",
    "        # print(res, mean)\n",
    "        sig = torch.sigmoid(torch.flatten(res))\n",
    "        return sig\n",
    "\n",
    "def encode_input(text, tokenizer):\n",
    "    max_length = 512\n",
    "    input = tokenizer(text, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "#     print(input.keys())\n",
    "    return input.input_ids, input.attention_mask\n",
    "    \n",
    "\n",
    "class BertGGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, gated_graph_conv_args, conv_args, emb_size, device):\n",
    "        super(BertGGCN, self).__init__()\n",
    "        self.k = 0.3\n",
    "        self.ggc = GatedGraphConv(**gated_graph_conv_args).to(device)\n",
    "        self.conv = Conv(**conv_args,\n",
    "                         fc_1_size=gated_graph_conv_args[\"out_channels\"] + emb_size,\n",
    "                         fc_2_size=gated_graph_conv_args[\"out_channels\"]).to(device)\n",
    "        self.nb_class = 2\n",
    "        # self.conv.apply(init_weights)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # the DataLoader format\n",
    "        # DataBatch(x=[1640, 101], edge_index=[2, 933], y=[8], func=[8], batch=[1640], ptr=[9])\n",
    "        \n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.ggc(x, edge_index)\n",
    "        x = self.conv(x, data.x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save(self, path):\n",
    "        print(path)\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(\"save!!!!!!\")\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "def softmax_accuracy(probs, all_labels):\n",
    "    acc = (torch.argmax(probs) == all_labels).sum()\n",
    "    acc = torch.div(acc, len(all_labels) + 0.0)\n",
    "    return acc\n",
    "\n",
    "class Stat:\n",
    "    def __init__(self, outs=None, loss=0.0, acc=0.0, labels=None):\n",
    "        if labels is None:\n",
    "            labels = []\n",
    "        if outs is None:\n",
    "            outs = []\n",
    "        self.outs = outs\n",
    "        self.labels = labels\n",
    "        self.loss = loss\n",
    "        self.acc = acc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Stat(self.outs + other.outs, self.loss + other.loss, self.acc + other.acc, self.labels + other.labels)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Loss: {round(self.loss, 4)}; Acc: {round(self.acc, 4)};\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Stats:\n",
    "    name: str\n",
    "    results: List[Stat] = dataclasses.field(default_factory=list)\n",
    "    total: Stat = Stat()\n",
    "\n",
    "    def __call__(self, stat):\n",
    "        self.total += stat\n",
    "        self.results.append(stat)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name} {self.mean()}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "\n",
    "    def mean(self):\n",
    "        res = Stat()\n",
    "        res += self.total\n",
    "        res.loss /= len(self)\n",
    "        res.acc /= len(self)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def loss(self):\n",
    "        return self.mean().loss\n",
    "\n",
    "    def acc(self):\n",
    "        return self.mean().acc\n",
    "\n",
    "    def outs(self):\n",
    "        return self.total.outs\n",
    "\n",
    "    def labels(self):\n",
    "        return self.total.labels\n",
    "\n",
    "\n",
    "class Step:\n",
    "    # Performs a step on the loader and returns the result\n",
    "    def __init__(self, model, loss_function, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = loss_function\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __call__(self, i, x, y):\n",
    "        out = self.model(x)\n",
    "        # y= y.type(torch.LongTensor)\n",
    "        loss = self.criterion(out, y.float())\n",
    "        acc = softmax_accuracy(out, y.float())\n",
    "\n",
    "        if self.model.training:\n",
    "            # calculates the gradient\n",
    "            loss.backward()\n",
    "            # and performs a parameter update based on it\n",
    "            self.optimizer.step()\n",
    "            # clears old gradients from the last step\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"\\tBatch: {i}; Loss: {round(loss.item(), 4)}\", end=\"\")\n",
    "        return Stat(out.tolist(), loss.item(), acc.item(), y.tolist())\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "class LoaderStep:\n",
    "    def __init__(self, name, data_loader, device):\n",
    "        self.name = name\n",
    "        self.loader = data_loader\n",
    "        self.size = len(data_loader)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, step):\n",
    "        self.stats = Stats(self.name)\n",
    "\n",
    "        for i, batch in enumerate(self.loader):\n",
    "            batch.to(self.device)\n",
    "            stat: Stat = step(i, batch, batch.y)\n",
    "            self.stats(stat)\n",
    "\n",
    "        return self.stats\n",
    "\n",
    "class Devign(Step):\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 device: str,\n",
    "                 model: dict,\n",
    "                 learning_rate: float,\n",
    "                 weight_decay: float,\n",
    "                 loss_lambda: float):\n",
    "        self.path = path\n",
    "        self.lr = learning_rate\n",
    "        self.wd = weight_decay\n",
    "        self.ll = loss_lambda\n",
    "        log_info('devign', f\"LR: {self.lr}; WD: {self.wd}; LL: {self.ll};\")\n",
    "        _model = BertGGCN(**model, device=device)\n",
    "        super().__init__(model=_model,\n",
    "                         loss_function=lambda o, t: F.binary_cross_entropy(o, t) + F.l1_loss(o, t) * self.ll,\n",
    "                         optimizer=optim.Adam(_model.parameters(), lr=self.lr, weight_decay=self.wd),\n",
    "                         )\n",
    "\n",
    "\n",
    "        self.count_parameters()\n",
    "\n",
    "    def load(self):\n",
    "        self.model.load(self.path)\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save(self.path)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        count = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"The model has {count:,} trainable parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b826580e-3788-4505-a2c8-3a718be5d070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, outs, labels):\n",
    "        self.scores = outs\n",
    "        self.labels = labels\n",
    "        self.transform()\n",
    "        print(self.predicts)\n",
    "\n",
    "    def transform(self):\n",
    "        self.series = pd.Series(self.scores)\n",
    "        self.predicts = self.series.apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        self.predicts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __str__(self):\n",
    "        confusion = confusion_matrix(y_true=self.labels, y_pred=self.predicts)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        string = f\"\\nConfusion matrix: \\n\"\n",
    "        string += f\"{confusion}\\n\"\n",
    "        string += f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\\n\"\n",
    "        string += '\\n'.join([name + \": \" + str(metric) for name, metric in self().items()])\n",
    "        return string\n",
    "\n",
    "    def __call__(self):\n",
    "        _metrics = {\"Accuracy\": metrics.accuracy_score(y_true=self.labels, y_pred=self.predicts),\n",
    "                    \"Precision\": metrics.precision_score(y_true=self.labels, y_pred=self.predicts),\n",
    "                    \"Recall\": metrics.recall_score(y_true=self.labels, y_pred=self.predicts),\n",
    "                    \"F-measure\": metrics.f1_score(y_true=self.labels, y_pred=self.predicts),\n",
    "                    \"Precision-Recall AUC\": metrics.average_precision_score(y_true=self.labels, y_score=self.scores),\n",
    "                    \"AUC\": metrics.roc_auc_score(y_true=self.labels, y_score=self.scores),\n",
    "                    \"MCC\": metrics.matthews_corrcoef(y_true=self.labels, y_pred=self.predicts),\n",
    "                    \"Error\": self.error()}\n",
    "\n",
    "        return _metrics\n",
    "\n",
    "    def log(self):\n",
    "        excluded = [\"Precision-Recall AUC\", \"AUC\"]\n",
    "        _metrics = self()\n",
    "        rounded_metrics = {name: torch.round(torch.tensor(metric) * 1000) / 1000 for name, metric in _metrics.items() if\n",
    "                           name not in excluded}\n",
    "        msg = ' - '.join([f\"({name[:3]} {round(metric.item(), 3)})\" for name, metric in rounded_metrics.items()])\n",
    "\n",
    "        # logger.log_info('metrics', msg)\n",
    "\n",
    "    def error(self):\n",
    "        errors = [(abs(score - (1 if score >= 0.5 else 0))/score)*100 for score, label in zip(self.scores, self.labels)]\n",
    "\n",
    "        return sum(errors)/len(errors)\n",
    "\n",
    "class Train(object):\n",
    "    def __init__(self, step, epochs, verbose=True):\n",
    "        self.epochs = epochs\n",
    "        self.step = step\n",
    "        self.history = History()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, train_loader_step, val_loader_step=None, early_stopping=None):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.step.train()\n",
    "            train_stats = train_loader_step(self.step)\n",
    "            self.history(train_stats, epoch + 1)\n",
    "\n",
    "            if val_loader_step is not None:\n",
    "                with torch.no_grad():\n",
    "                    self.step.eval()\n",
    "                    val_stats = val_loader_step(self.step)\n",
    "                    self.history(val_stats, epoch + 1)\n",
    "\n",
    "                print(self.history)\n",
    "\n",
    "                if early_stopping is not None:\n",
    "                    valid_loss = val_stats.loss()\n",
    "                    # early_stopping needs the validation loss to check if it has decreased,\n",
    "                    # and if it has, it will make a checkpoint of the current model\n",
    "                    if early_stopping(valid_loss):\n",
    "                        self.history.log()\n",
    "                        return\n",
    "            else:\n",
    "                print(self.history)\n",
    "        self.history.log()\n",
    "\n",
    "\n",
    "def predict(step, test_loader_step):\n",
    "    print(f\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        step.eval()\n",
    "        stats = test_loader_step(step)\n",
    "        metrics = Metrics(stats.outs(), stats.labels())\n",
    "        print(metrics)\n",
    "        metrics.log()\n",
    "    return metrics()[\"Accuracy\"]\n",
    "\n",
    "\n",
    "class History:\n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "        self.epoch = 0\n",
    "        self.timer = time.time()\n",
    "\n",
    "    def __call__(self, stats, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "        if epoch in self.history:\n",
    "            self.history[epoch].append(stats)\n",
    "        else:\n",
    "            self.history[epoch] = [stats]\n",
    "\n",
    "    def __str__(self):\n",
    "        epoch = f\"\\nEpoch {self.epoch};\"\n",
    "        stats = ' - '.join([f\"{res}\" for res in self.current()])\n",
    "        timer = f\"Time: {(time.time() - self.timer)}\"\n",
    "\n",
    "        return f\"{epoch} - {stats} - {timer}\"\n",
    "\n",
    "    def current(self):\n",
    "        return self.history[self.epoch]\n",
    "\n",
    "    def log(self):\n",
    "        msg = f\"(Epoch: {self.epoch}) {' - '.join([f'({res})' for res in self.current()])}\"\n",
    "        log_info(\"history\", msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a169f550-000f-43b5-bd67-993f324191a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, model, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(\"Early stopping\")\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        \"\"\"\n",
    "            Saves model when validation loss decrease.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.model.save()\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63599b3d-db37-4224-8282-63e17102d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os.path\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def embed_task():\n",
    "    context = Embed()\n",
    "    # Tokenize source code into tokens\n",
    "    dataset_files = get_directory_files(PATHS.cpg)\n",
    "    w2vmodel = Word2Vec(**context.w2v_args)\n",
    "    w2v_init = True\n",
    "    for pkl_file in dataset_files:\n",
    "        file_name = pkl_file.split(\".\")[0]\n",
    "        cpg_dataset = load(PATHS.cpg, pkl_file)\n",
    "        tokens_dataset = tokenize(cpg_dataset)\n",
    "        write(tokens_dataset, PATHS.tokens, f\"{file_name}_{FILES.tokens}\")\n",
    "        # word2vec used to learn the initial embedding of each token\n",
    "        w2vmodel.build_vocab(tokens_dataset.tokens, update=not w2v_init)\n",
    "        w2vmodel.train(tokens_dataset.tokens, total_examples=w2vmodel.corpus_count, epochs=1)\n",
    "        if w2v_init:\n",
    "            w2v_init = False\n",
    "\n",
    "        # cpg_dataset = load(PATHS.cpg, pkl_file)\n",
    "        # Embed cpg to node representation and pass to graph data structure\n",
    "        cpg_dataset[\"nodes\"] = cpg_dataset.apply(lambda row: parse_to_nodes(row.cpg, context.nodes_dim), axis=1)\n",
    "        # remove rows with no nodes\n",
    "        # Use key_to_index instead of vocab\n",
    "        cpg_dataset[\"input\"] = cpg_dataset.apply(lambda row: nodes_to_input(row.nodes, row.target, context.nodes_dim,\n",
    "                                  w2vmodel.wv, context.edge_type), axis=1)\n",
    "\n",
    "        drop(cpg_dataset, [\"nodes\"])\n",
    "        print(f\"Saving input dataset {file_name} with size {len(cpg_dataset)}.\")\n",
    "        # write(cpg_dataset[[\"input\", \"target\"]], PATHS.input, f\"{file_name}_{FILES.input}\")\n",
    "\n",
    "        # 为适应 codebert 的输入，我们需要额外的保存一行 func\n",
    "        # write(cpg_dataset[[\"input\", \"target\",\"func\"]], PATHS.input, f\"{file_name}_{FILES.input}\")\n",
    "        write(cpg_dataset[[\"input\", \"target\", \"func\"]], PATHS.input, f\"{file_name}_{FILES.input}\")\n",
    "        \n",
    "        del cpg_dataset\n",
    "        gc.collect()\n",
    "    print(\"Saving w2vmodel.\")\n",
    "    # w2vmodel.save(f\"{PATHS.w2v}/{FILES.w2v}\")\n",
    "\n",
    "\n",
    "def process_task(stopping):\n",
    "    context = Process()\n",
    "    devign = Devign_class()\n",
    "    # model_path = PATHS.model + FILES.model\n",
    "\n",
    "    model_path =\"Codebert_ggnn_verison_1.pth\"\n",
    "    model = Devign(path=model_path, device=DEVICE, model=devign.model, learning_rate=devign.learning_rate,\n",
    "                           weight_decay=devign.weight_decay,\n",
    "                           loss_lambda=devign.loss_lambda)\n",
    "    train = Train(model, context.epochs)\n",
    "    input_dataset = loads(PATHS.input)\n",
    "    # split the dataset and pass to DataLoader with batch size\n",
    "    train_loader, val_loader, test_loader = list(\n",
    "        map(lambda x: x.get_loader(context.batch_size, shuffle=context.shuffle),\n",
    "            train_val_test_split(input_dataset, shuffle=context.shuffle)))\n",
    "    train_loader_step = LoaderStep(\"Train\", train_loader, DEVICE)\n",
    "    val_loader_step = LoaderStep(\"Validation\", val_loader, DEVICE)\n",
    "    test_loader_step = LoaderStep(\"Test\", test_loader, DEVICE)\n",
    "\n",
    "    if stopping:\n",
    "        early_stopping = EarlyStopping(model, patience=context.patience)\n",
    "        train(train_loader_step, val_loader_step, early_stopping)\n",
    "        model.load()\n",
    "    else:\n",
    "        train(train_loader_step, val_loader_step)\n",
    "        print(\"Saving modeling !!!\")\n",
    "        model.save()\n",
    "\n",
    "    predict(model, test_loader_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5d81dbf-9a2a-4cd5-b0a6-bd4327b8b722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 900 to 1199\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  298 non-null    int64 \n",
      " 1   func    298 non-null    object\n",
      " 2   Index   298 non-null    int64 \n",
      " 3   cpg     298 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 264.9 KB\n",
      "CPG cut - original nodes: 237 to max: 205\n",
      "CPG cut - original nodes: 212 to max: 205\n",
      "Saving input dataset 3_cpg with size 298.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 2100 to 2399\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  297 non-null    int64 \n",
      " 1   func    297 non-null    object\n",
      " 2   Index   297 non-null    int64 \n",
      " 3   cpg     297 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 265.1 KB\n",
      "Saving input dataset 7_cpg with size 297.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 294 entries, 2700 to 2999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  294 non-null    int64 \n",
      " 1   func    294 non-null    object\n",
      " 2   Index   294 non-null    int64 \n",
      " 3   cpg     294 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 267.0 KB\n",
      "CPG cut - original nodes: 246 to max: 205\n",
      "CPG cut - original nodes: 281 to max: 205\n",
      "CPG cut - original nodes: 287 to max: 205\n",
      "CPG cut - original nodes: 216 to max: 205\n",
      "Saving input dataset 9_cpg with size 294.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 0 to 299\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  297 non-null    int64 \n",
      " 1   func    297 non-null    object\n",
      " 2   Index   297 non-null    int64 \n",
      " 3   cpg     297 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 270.2 KB\n",
      "CPG cut - original nodes: 266 to max: 205\n",
      "CPG cut - original nodes: 206 to max: 205\n",
      "Saving input dataset 0_cpg with size 297.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 600 to 899\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  298 non-null    int64 \n",
      " 1   func    298 non-null    object\n",
      " 2   Index   298 non-null    int64 \n",
      " 3   cpg     298 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 270.9 KB\n",
      "CPG cut - original nodes: 342 to max: 205\n",
      "CPG cut - original nodes: 328 to max: 205\n",
      "Saving input dataset 2_cpg with size 298.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 3300 to 3599\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  297 non-null    int64 \n",
      " 1   func    297 non-null    object\n",
      " 2   Index   297 non-null    int64 \n",
      " 3   cpg     297 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 263.4 KB\n",
      "CPG cut - original nodes: 206 to max: 205\n",
      "CPG cut - original nodes: 233 to max: 205\n",
      "CPG cut - original nodes: 229 to max: 205\n",
      "Saving input dataset 11_cpg with size 297.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 1500 to 1799\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  298 non-null    int64 \n",
      " 1   func    298 non-null    object\n",
      " 2   Index   298 non-null    int64 \n",
      " 3   cpg     298 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 260.4 KB\n",
      "CPG cut - original nodes: 236 to max: 205\n",
      "Saving input dataset 5_cpg with size 298.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 296 entries, 1200 to 1499\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  296 non-null    int64 \n",
      " 1   func    296 non-null    object\n",
      " 2   Index   296 non-null    int64 \n",
      " 3   cpg     296 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 257.3 KB\n",
      "CPG cut - original nodes: 214 to max: 205\n",
      "CPG cut - original nodes: 228 to max: 205\n",
      "Saving input dataset 4_cpg with size 296.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 301 entries, 3000 to 3299\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  301 non-null    int64 \n",
      " 1   func    301 non-null    object\n",
      " 2   Index   301 non-null    int64 \n",
      " 3   cpg     301 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 272.6 KB\n",
      "CPG cut - original nodes: 226 to max: 205\n",
      "CPG cut - original nodes: 227 to max: 205\n",
      "CPG cut - original nodes: 270 to max: 205\n",
      "Saving input dataset 10_cpg with size 301.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 300 entries, 300 to 599\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  300 non-null    int64 \n",
      " 1   func    300 non-null    object\n",
      " 2   Index   300 non-null    int64 \n",
      " 3   cpg     300 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 271.3 KB\n",
      "CPG cut - original nodes: 290 to max: 205\n",
      "Saving input dataset 1_cpg with size 300.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 310 entries, 2400 to 2699\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  310 non-null    int64 \n",
      " 1   func    310 non-null    object\n",
      " 2   Index   310 non-null    int64 \n",
      " 3   cpg     310 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 273.7 KB\n",
      "Saving input dataset 8_cpg with size 310.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 276 entries, 3600 to 3876\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  276 non-null    int64 \n",
      " 1   func    276 non-null    object\n",
      " 2   Index   276 non-null    int64 \n",
      " 3   cpg     276 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 243.0 KB\n",
      "CPG cut - original nodes: 226 to max: 205\n",
      "CPG cut - original nodes: 209 to max: 205\n",
      "Saving input dataset 12_cpg with size 276.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 1800 to 2099\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  298 non-null    int64 \n",
      " 1   func    298 non-null    object\n",
      " 2   Index   298 non-null    int64 \n",
      " 3   cpg     298 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 276.8 KB\n",
      "CPG cut - original nodes: 230 to max: 205\n",
      "Saving input dataset 6_cpg with size 298.\n",
      "Saving w2vmodel.\n"
     ]
    }
   ],
   "source": [
    "embed_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd3fc804-2c25-4e31-9e2d-9914ea9d5311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 515,542 trainable parameters\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 0 to 299\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   297 non-null    object\n",
      " 1   target  297 non-null    int64 \n",
      " 2   func    297 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 214.5 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 301 entries, 3000 to 3299\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   301 non-null    object\n",
      " 1   target  301 non-null    int64 \n",
      " 2   func    301 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 216.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 3300 to 3599\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   297 non-null    object\n",
      " 1   target  297 non-null    int64 \n",
      " 2   func    297 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 207.7 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 276 entries, 3600 to 3876\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   276 non-null    object\n",
      " 1   target  276 non-null    int64 \n",
      " 2   func    276 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 191.3 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 300 entries, 300 to 599\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   300 non-null    object\n",
      " 1   target  300 non-null    int64 \n",
      " 2   func    300 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 215.0 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 600 to 899\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   298 non-null    object\n",
      " 1   target  298 non-null    int64 \n",
      " 2   func    298 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 215.0 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 900 to 1199\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   298 non-null    object\n",
      " 1   target  298 non-null    int64 \n",
      " 2   func    298 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 209.0 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 296 entries, 1200 to 1499\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   296 non-null    object\n",
      " 1   target  296 non-null    int64 \n",
      " 2   func    296 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 201.8 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 1500 to 1799\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   298 non-null    object\n",
      " 1   target  298 non-null    int64 \n",
      " 2   func    298 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 204.5 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 298 entries, 1800 to 2099\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   298 non-null    object\n",
      " 1   target  298 non-null    int64 \n",
      " 2   func    298 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 220.9 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 2100 to 2399\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   297 non-null    object\n",
      " 1   target  297 non-null    int64 \n",
      " 2   func    297 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 209.4 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 310 entries, 2400 to 2699\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   310 non-null    object\n",
      " 1   target  310 non-null    int64 \n",
      " 2   func    310 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 215.5 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 294 entries, 2700 to 2999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   294 non-null    object\n",
      " 1   target  294 non-null    int64 \n",
      " 2   func    294 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 211.9 KB\n",
      "Splitting Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1; - Train Loss: 0.6933; Acc: 0.1665; - Validation Loss: 0.6903; Acc: 0.1122; - Time: 6.619290828704834\n",
      "Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 2; - Train Loss: 0.6912; Acc: 0.1285; - Validation Loss: 0.6934; Acc: 0.1199; - Time: 9.893531560897827\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch 3; - Train Loss: 0.6836; Acc: 0.1362; - Validation Loss: 0.6998; Acc: 0.1224; - Time: 13.108749866485596\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Epoch 4; - Train Loss: 0.6616; Acc: 0.1344; - Validation Loss: 0.7412; Acc: 0.0612; - Time: 16.382174968719482\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Epoch 5; - Train Loss: 0.6296; Acc: 0.1274; - Validation Loss: 0.7873; Acc: 0.1378; - Time: 19.71784734725952\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Epoch 6; - Train Loss: 0.5965; Acc: 0.1315; - Validation Loss: 0.7699; Acc: 0.1531; - Time: 23.188239812850952\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Epoch 7; - Train Loss: 0.5589; Acc: 0.1108; - Validation Loss: 0.8558; Acc: 0.0689; - Time: 26.71869921684265\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Epoch 8; - Train Loss: 0.52; Acc: 0.137; - Validation Loss: 0.8648; Acc: 0.1403; - Time: 29.948318004608154\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Epoch 9; - Train Loss: 0.4788; Acc: 0.1373; - Validation Loss: 1.0208; Acc: 0.0714; - Time: 33.23230695724487\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Epoch 10; - Train Loss: 0.4335; Acc: 0.1312; - Validation Loss: 1.095; Acc: 0.1301; - Time: 36.499831438064575\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Epoch 11; - Train Loss: 0.4006; Acc: 0.129; - Validation Loss: 1.1369; Acc: 0.1658; - Time: 39.7635543346405\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Testing\n",
      "0      1\n",
      "1      0\n",
      "2      1\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "382    0\n",
      "383    0\n",
      "384    1\n",
      "385    0\n",
      "386    1\n",
      "Length: 387, dtype: int64\n",
      "\n",
      "Confusion matrix: \n",
      "[[155  43]\n",
      " [140  49]]\n",
      "TP: 49, FP: 43, TN: 155, FN: 140\n",
      "Accuracy: 0.5271317829457365\n",
      "Precision: 0.532608695652174\n",
      "Recall: 0.25925925925925924\n",
      "F-measure: 0.34875444839857656\n",
      "Precision-Recall AUC: 0.5206063386783468\n",
      "AUC: 0.5499171610282722\n",
      "MCC: 0.04942106946580228\n",
      "Error: 98.42127480019273\n"
     ]
    }
   ],
   "source": [
    "process_task(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad565c3-3ecf-42fb-a417-35fcec115f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 125,162,712 trainable parameters\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497 entries, 2 to 3330\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   497 non-null    object\n",
      " 1   target  497 non-null    int64 \n",
      " 2   func    497 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 345.9 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497 entries, 3331 to 6789\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   497 non-null    object\n",
      " 1   target  497 non-null    int64 \n",
      " 2   func    497 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 366.3 KB\n",
      "Splitting Dataset\n",
      "\n",
      "Epoch 1; - Train Loss: 0.9325; Acc: 0.16; - Validation Loss: 0.698; Acc: 0.0769; - Time: 3.0846588611602783\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 2; - Train Loss: 0.6988; Acc: 0.18; - Validation Loss: 0.6947; Acc: 0.0769; - Time: 6.247617483139038\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 3; - Train Loss: 0.6962; Acc: 0.18; - Validation Loss: 0.6936; Acc: 0.0; - Time: 9.402512311935425\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 4; - Train Loss: 0.6946; Acc: 0.16; - Validation Loss: 0.6931; Acc: 0.0; - Time: 12.547507047653198\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 5; - Train Loss: 0.6941; Acc: 0.16; - Validation Loss: 0.6929; Acc: 0.0; - Time: 15.722753047943115\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 6; - Train Loss: 0.6934; Acc: 0.13; - Validation Loss: 0.693; Acc: 0.0; - Time: 18.901556253433228\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch 7; - Train Loss: 0.6927; Acc: 0.12; - Validation Loss: 0.6929; Acc: 0.0; - Time: 21.166598320007324\n",
      "/hy-tmp/data/model/Codebert_ggnn_verison_1.pth\n",
      "save!!!!!!\n",
      "\n",
      "Epoch 8; - Train Loss: 0.693; Acc: 0.18; - Validation Loss: 0.6931; Acc: 0.0; - Time: 24.35701894760132\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Epoch 9; - Train Loss: 0.6927; Acc: 0.12; - Validation Loss: 0.6933; Acc: 0.0; - Time: 26.63476872444153\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Epoch 10; - Train Loss: 0.6912; Acc: 0.13; - Validation Loss: 0.6932; Acc: 0.0; - Time: 28.898579120635986\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Epoch 11; - Train Loss: 0.6925; Acc: 0.15; - Validation Loss: 0.6937; Acc: 0.0; - Time: 31.174543380737305\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Epoch 12; - Train Loss: 0.6905; Acc: 0.16; - Validation Loss: 0.6941; Acc: 0.0; - Time: 33.41719627380371\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Epoch 13; - Train Loss: 0.6884; Acc: 0.16; - Validation Loss: 0.6951; Acc: 0.0; - Time: 35.58753943443298\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Epoch 14; - Train Loss: 0.689; Acc: 0.16; - Validation Loss: 0.6959; Acc: 0.0; - Time: 37.82253694534302\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Epoch 15; - Train Loss: 0.6863; Acc: 0.12; - Validation Loss: 0.7007; Acc: 0.0; - Time: 40.05801224708557\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Epoch 16; - Train Loss: 0.6826; Acc: 0.12; - Validation Loss: 0.704; Acc: 0.0; - Time: 42.278186321258545\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Epoch 17; - Train Loss: 0.6758; Acc: 0.1; - Validation Loss: 0.7108; Acc: 0.0; - Time: 44.52412414550781\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "Testing\n",
      "0      1\n",
      "1      1\n",
      "2      0\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "96     1\n",
      "97     1\n",
      "98     1\n",
      "99     1\n",
      "100    1\n",
      "Length: 101, dtype: int64\n",
      "\n",
      "Confusion matrix: \n",
      "[[18 31]\n",
      " [12 40]]\n",
      "TP: 40, FP: 31, TN: 18, FN: 12\n",
      "Accuracy: 0.5742574257425742\n",
      "Precision: 0.5633802816901409\n",
      "Recall: 0.7692307692307693\n",
      "F-measure: 0.6504065040650407\n",
      "Precision-Recall AUC: 0.541145210644498\n",
      "AUC: 0.5639717425431711\n",
      "MCC: 0.1493790628122286\n",
      "Error: 98.64423059325756\n"
     ]
    }
   ],
   "source": [
    "process_task(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72a5c6-f511-4245-80f1-e69047d6688d",
   "metadata": {},
   "source": [
    "## Details of embed_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8465f88-8620-45cb-95b3-cbb79d352c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = Embed()\n",
    "# Tokenize source code into tokens\n",
    "dataset_files = get_directory_files(PATHS.cpg)\n",
    "w2vmodel = Word2Vec(**context.w2v_args)\n",
    "w2v_init = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3a29e83-1f30-490a-a504-46cea0accacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_cpg.pkl', '1_cpg.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fa5a8b6-f7ca-4247-966a-e26805e88ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497 entries, 2 to 3330\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  497 non-null    int64 \n",
      " 1   func    497 non-null    object\n",
      " 2   Index   497 non-null    int64 \n",
      " 3   cpg     497 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 439.0 KB\n"
     ]
    }
   ],
   "source": [
    "pkl_file = dataset_files[0]\n",
    "\n",
    "def tokenize(data_frame: pd.DataFrame):\n",
    "    data_frame.func = data_frame.func.apply(tokenizer)\n",
    "    # Change column name\n",
    "    data_frame = rename(data_frame, 'func', 'tokens')\n",
    "    # Keep just the tokens\n",
    "    return data_frame[[\"tokens\"]]\n",
    "    \n",
    "file_name = pkl_file.split(\".\")[0]\n",
    "cpg_dataset = load(PATHS.cpg, pkl_file)\n",
    "tokens_dataset = tokenize(cpg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "269b70e6-25d0-49af-80e3-63b9816b5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>func</th>\n",
       "      <th>Index</th>\n",
       "      <th>cpg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[static, void, FUN1, (, void, *, VAR1, ,, uint...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'functions': [{'function': 'v4l2_free_buffer'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'functions': [{'function': 'av_opencl_buffer_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>[static, int, FUN1, (, AVFormatContext, *, VAR...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'functions': [{'function': 'r3d_read_rdvo', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[void, FUN1, (, AVDictionary, *, VAR1, ), {, A...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'functions': [{'function': 'assert_avoptions'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[av_cold, void, FUN1, (, AVCodecContext, *, VA...</td>\n",
       "      <td>19</td>\n",
       "      <td>{'functions': [{'function': 'ff_af_queue_init'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>0</td>\n",
       "      <td>[static, int, FUN1, (, FTPContext, *, VAR1, ),...</td>\n",
       "      <td>3315</td>\n",
       "      <td>{'functions': [{'function': 'ftp_flush_control...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>1</td>\n",
       "      <td>[int64_t, FUN1, (, int64_t, VAR1, ,, int64_t, ...</td>\n",
       "      <td>3323</td>\n",
       "      <td>{'functions': [{'function': 'av_gcd', 'id': 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>1</td>\n",
       "      <td>[static, inline, int, FUN1, (, APEContext, *, ...</td>\n",
       "      <td>3324</td>\n",
       "      <td>{'functions': [{'function': 'ape_decode_value_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>1</td>\n",
       "      <td>[int, FUN1, (, AVCodecContext, *, VAR1, ,, AVP...</td>\n",
       "      <td>3325</td>\n",
       "      <td>{'functions': [{'function': 'avcodec_decode_vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0</td>\n",
       "      <td>[static, void, av_always_inline, FUN1, (, uint...</td>\n",
       "      <td>3330</td>\n",
       "      <td>{'functions': [{'function': 'filter_mb_edgech'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               func  Index  \\\n",
       "2          0  [static, void, FUN1, (, void, *, VAR1, ,, uint...      2   \n",
       "4          0  [int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...      4   \n",
       "5          1  [static, int, FUN1, (, AVFormatContext, *, VAR...      5   \n",
       "11         1  [void, FUN1, (, AVDictionary, *, VAR1, ), {, A...     11   \n",
       "19         0  [av_cold, void, FUN1, (, AVCodecContext, *, VA...     19   \n",
       "...      ...                                                ...    ...   \n",
       "3315       0  [static, int, FUN1, (, FTPContext, *, VAR1, ),...   3315   \n",
       "3323       1  [int64_t, FUN1, (, int64_t, VAR1, ,, int64_t, ...   3323   \n",
       "3324       1  [static, inline, int, FUN1, (, APEContext, *, ...   3324   \n",
       "3325       1  [int, FUN1, (, AVCodecContext, *, VAR1, ,, AVP...   3325   \n",
       "3330       0  [static, void, av_always_inline, FUN1, (, uint...   3330   \n",
       "\n",
       "                                                    cpg  \n",
       "2     {'functions': [{'function': 'v4l2_free_buffer'...  \n",
       "4     {'functions': [{'function': 'av_opencl_buffer_...  \n",
       "5     {'functions': [{'function': 'r3d_read_rdvo', '...  \n",
       "11    {'functions': [{'function': 'assert_avoptions'...  \n",
       "19    {'functions': [{'function': 'ff_af_queue_init'...  \n",
       "...                                                 ...  \n",
       "3315  {'functions': [{'function': 'ftp_flush_control...  \n",
       "3323  {'functions': [{'function': 'av_gcd', 'id': 'n...  \n",
       "3324  {'functions': [{'function': 'ape_decode_value_...  \n",
       "3325  {'functions': [{'function': 'avcodec_decode_vi...  \n",
       "3330  {'functions': [{'function': 'filter_mb_edgech'...  \n",
       "\n",
       "[497 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4254cd2b-39a4-4b0f-92e5-945c8f1ecb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[static, void, FUN1, (, void, *, VAR1, ,, uint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[static, int, FUN1, (, AVFormatContext, *, VAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[void, FUN1, (, AVDictionary, *, VAR1, ), {, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[av_cold, void, FUN1, (, AVCodecContext, *, VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>[static, int, FUN1, (, FTPContext, *, VAR1, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>[int64_t, FUN1, (, int64_t, VAR1, ,, int64_t, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>[static, inline, int, FUN1, (, APEContext, *, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>[int, FUN1, (, AVCodecContext, *, VAR1, ,, AVP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>[static, void, av_always_inline, FUN1, (, uint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens\n",
       "2     [static, void, FUN1, (, void, *, VAR1, ,, uint...\n",
       "4     [int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...\n",
       "5     [static, int, FUN1, (, AVFormatContext, *, VAR...\n",
       "11    [void, FUN1, (, AVDictionary, *, VAR1, ), {, A...\n",
       "19    [av_cold, void, FUN1, (, AVCodecContext, *, VA...\n",
       "...                                                 ...\n",
       "3315  [static, int, FUN1, (, FTPContext, *, VAR1, ),...\n",
       "3323  [int64_t, FUN1, (, int64_t, VAR1, ,, int64_t, ...\n",
       "3324  [static, inline, int, FUN1, (, APEContext, *, ...\n",
       "3325  [int, FUN1, (, AVCodecContext, *, VAR1, ,, AVP...\n",
       "3330  [static, void, av_always_inline, FUN1, (, uint...\n",
       "\n",
       "[497 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9d2b669-f43b-417b-a7b4-c67001a163a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPG cut - original nodes: 264 to max: 205\n",
      "CPG cut - original nodes: 263 to max: 205\n",
      "CPG cut - original nodes: 236 to max: 205\n",
      "CPG cut - original nodes: 275 to max: 205\n",
      "CPG cut - original nodes: 215 to max: 205\n",
      "CPG cut - original nodes: 215 to max: 205\n"
     ]
    }
   ],
   "source": [
    "w2vmodel.build_vocab(tokens_dataset.tokens, update=not w2v_init)\n",
    "w2vmodel.train(tokens_dataset.tokens, total_examples=w2vmodel.corpus_count, epochs=1)\n",
    "if w2v_init:\n",
    "    w2v_init = False\n",
    "# Embed cpg to node representation and pass to graph data structure\n",
    "cpg_dataset[\"nodes\"] = cpg_dataset.apply(lambda row: parse_to_nodes(row.cpg, context.nodes_dim), axis=1)\n",
    "# remove rows with no nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fac294e-e4ed-4edb-9718-cd44fb8b12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use key_to_index instead of vocab\n",
    "cpg_dataset[\"input\"] = cpg_dataset.apply(lambda row: nodes_to_input(row.nodes, row.target, context.nodes_dim,\n",
    "                          w2vmodel.wv, context.edge_type), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3766829d-63d5-46af-a0fc-b564bd6b02c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>func</th>\n",
       "      <th>Index</th>\n",
       "      <th>cpg</th>\n",
       "      <th>nodes</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, int, VAR1, ,, int, VAR2, ,, int, VAR...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'functions': [{'function': 'clear_area', 'id'...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=1808504...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, VAR1, *, VAR2, ), {, VAR1, *, VAR3, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'functions': [{'function': 'ReconstructDuList...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=2712756...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, void, ), {, if, (, VAR1, ), FUN2, (,...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'functions': [{'function': 'free_speaker', 'i...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=1265953...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, mlx4_dev, *, VAR1, ), {, str...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'functions': [{'function': 'mlx4_register_dev...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=2260630...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, void, ), {, char, *, VAR1, =, FUN2, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'functions': [{'function': 'Parse_Env_Var', '...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=3255307...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, tree, VAR1, ,, gimple, VAR2, ,, bool...</td>\n",
       "      <td>95</td>\n",
       "      <td>{'functions': [{'function': 'create_access', '...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=8771245...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, VAR1, *, VAR2, ,, VAR3, *, VAR4, ,, ...</td>\n",
       "      <td>96</td>\n",
       "      <td>{'functions': [{'function': 'mailimap_body_ext...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=8861671...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, i2c_client, *, VAR1, ), {, u...</td>\n",
       "      <td>97</td>\n",
       "      <td>{'functions': [{'function': 'i2c_read_le16', '...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=8952096...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, int, VAR1, ), {, char, VAR2, [, 20, ...</td>\n",
       "      <td>98</td>\n",
       "      <td>{'functions': [{'function': 'draw_keys', 'id':...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=9042521...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, hns_mac_cb, *, VAR1, ), {, e...</td>\n",
       "      <td>99</td>\n",
       "      <td>{'functions': [{'function': 'hns_dsaf_fix_mac_...</td>\n",
       "      <td>{'MethodReturn[label=METHOD_RETURN; id=9132946...</td>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                                               func  Index  \\\n",
       "0        0  [FUN1, (, int, VAR1, ,, int, VAR2, ,, int, VAR...      0   \n",
       "1        0  [FUN1, (, VAR1, *, VAR2, ), {, VAR1, *, VAR3, ...      1   \n",
       "2        0  [FUN1, (, void, ), {, if, (, VAR1, ), FUN2, (,...      2   \n",
       "3        0  [FUN1, (, struct, mlx4_dev, *, VAR1, ), {, str...      3   \n",
       "4        1  [FUN1, (, void, ), {, char, *, VAR1, =, FUN2, ...      4   \n",
       "..     ...                                                ...    ...   \n",
       "95       0  [FUN1, (, tree, VAR1, ,, gimple, VAR2, ,, bool...     95   \n",
       "96       0  [FUN1, (, VAR1, *, VAR2, ,, VAR3, *, VAR4, ,, ...     96   \n",
       "97       0  [FUN1, (, struct, i2c_client, *, VAR1, ), {, u...     97   \n",
       "98       1  [FUN1, (, int, VAR1, ), {, char, VAR2, [, 20, ...     98   \n",
       "99       0  [FUN1, (, struct, hns_mac_cb, *, VAR1, ), {, e...     99   \n",
       "\n",
       "                                                  cpg  \\\n",
       "0   {'functions': [{'function': 'clear_area', 'id'...   \n",
       "1   {'functions': [{'function': 'ReconstructDuList...   \n",
       "2   {'functions': [{'function': 'free_speaker', 'i...   \n",
       "3   {'functions': [{'function': 'mlx4_register_dev...   \n",
       "4   {'functions': [{'function': 'Parse_Env_Var', '...   \n",
       "..                                                ...   \n",
       "95  {'functions': [{'function': 'create_access', '...   \n",
       "96  {'functions': [{'function': 'mailimap_body_ext...   \n",
       "97  {'functions': [{'function': 'i2c_read_le16', '...   \n",
       "98  {'functions': [{'function': 'draw_keys', 'id':...   \n",
       "99  {'functions': [{'function': 'hns_dsaf_fix_mac_...   \n",
       "\n",
       "                                                nodes  \\\n",
       "0   {'MethodReturn[label=METHOD_RETURN; id=1808504...   \n",
       "1   {'MethodReturn[label=METHOD_RETURN; id=2712756...   \n",
       "2   {'MethodReturn[label=METHOD_RETURN; id=1265953...   \n",
       "3   {'MethodReturn[label=METHOD_RETURN; id=2260630...   \n",
       "4   {'MethodReturn[label=METHOD_RETURN; id=3255307...   \n",
       "..                                                ...   \n",
       "95  {'MethodReturn[label=METHOD_RETURN; id=8771245...   \n",
       "96  {'MethodReturn[label=METHOD_RETURN; id=8861671...   \n",
       "97  {'MethodReturn[label=METHOD_RETURN; id=8952096...   \n",
       "98  {'MethodReturn[label=METHOD_RETURN; id=9042521...   \n",
       "99  {'MethodReturn[label=METHOD_RETURN; id=9132946...   \n",
       "\n",
       "                                                input  \n",
       "0   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "1   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "2   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "3   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "4   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "..                                                ...  \n",
       "95  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "96  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "97  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "98  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "99  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...  \n",
       "\n",
       "[111 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9690f7f9-771b-43bf-b346-5a5f59dfe923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, int, VAR1, ,, int, VAR2, ,, int, VAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, VAR1, *, VAR2, ), {, VAR1, *, VAR3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, void, ), {, if, (, VAR1, ), FUN2, (,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, mlx4_dev, *, VAR1, ), {, str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, void, ), {, char, *, VAR1, =, FUN2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, tree, VAR1, ,, gimple, VAR2, ,, bool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, VAR1, *, VAR2, ,, VAR3, *, VAR4, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, i2c_client, *, VAR1, ), {, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, int, VAR1, ), {, char, VAR2, [, 20, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, hns_mac_cb, *, VAR1, ), {, e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  target  \\\n",
       "0   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "1   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "2   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "3   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "4   [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       1   \n",
       "..                                                ...     ...   \n",
       "95  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "96  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "97  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "98  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       1   \n",
       "99  [(x, [tensor([ 6.6000e+01,  2.0672e-03,  9.782...       0   \n",
       "\n",
       "                                                 func  \n",
       "0   [FUN1, (, int, VAR1, ,, int, VAR2, ,, int, VAR...  \n",
       "1   [FUN1, (, VAR1, *, VAR2, ), {, VAR1, *, VAR3, ...  \n",
       "2   [FUN1, (, void, ), {, if, (, VAR1, ), FUN2, (,...  \n",
       "3   [FUN1, (, struct, mlx4_dev, *, VAR1, ), {, str...  \n",
       "4   [FUN1, (, void, ), {, char, *, VAR1, =, FUN2, ...  \n",
       "..                                                ...  \n",
       "95  [FUN1, (, tree, VAR1, ,, gimple, VAR2, ,, bool...  \n",
       "96  [FUN1, (, VAR1, *, VAR2, ,, VAR3, *, VAR4, ,, ...  \n",
       "97  [FUN1, (, struct, i2c_client, *, VAR1, ), {, u...  \n",
       "98  [FUN1, (, int, VAR1, ), {, char, VAR2, [, 20, ...  \n",
       "99  [FUN1, (, struct, hns_mac_cb, *, VAR1, ), {, e...  \n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpg_dataset[[\"input\", \"target\",\"func\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1411fd-6ec5-429f-b136-ac4751101381",
   "metadata": {},
   "source": [
    "## Details of process_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c3b694b-9d7e-46ff-b7c6-ec5f3a2df57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hy-tmp/data/input/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = Process()\n",
    "data_sets_dir = PATHS.input\n",
    "data_sets_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67baaac6-d83e-43da-99fe-8819ab39be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets_files = sorted([f for f in listdir(data_sets_dir) if isfile(join(data_sets_dir, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36a41028-74c5-4d12-975f-d1100559c3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_cpg_input.pkl', '1_cpg_input.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "429bc728-865e-4e6c-b363-3a82b888089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, pickle_file, ratio=1):\n",
    "    dataset = pd.read_pickle(path + pickle_file)\n",
    "    # dataset.info(memory_usage='deep')\n",
    "    if ratio < 1:\n",
    "        dataset = get_ratio(dataset, ratio)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87fe59b1-6a35-4354-903b-bed4fa03786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 497 entries, 2 to 3330\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   497 non-null    object\n",
      " 1   target  497 non-null    int64 \n",
      " 2   func    497 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 662.1 KB\n"
     ]
    }
   ],
   "source": [
    "dataset = load(data_sets_dir, data_sets_files[0])\n",
    "data_sets_files.remove(data_sets_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9374fbd-ecb3-498e-b878-ce5466527609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[205, 101], edge_index=[2, 128], y=[1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data: 这表示该对象是一个 PyTorch Geometric 中的数据对象。\n",
    "# x=[205, 101]: x 是数据对象的特征矩阵，它的形状是 [205, 101]，意味着有 205 个样本（节点）和每个样本有 101 个特征。\n",
    "# edge_index=[2, 96]: edge_index 表示图的边索引，它的形状是 [2, 96]，表示图中有 96 条边，其中每条边由两个节点索引构成。\n",
    "# y=[1]: y 是目标变量，它的形状是 [1]，表示有一个目标值。\n",
    "\n",
    "dataset[\"input\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38d66f94-91cd-4ac4-92b8-21958fe76e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"input\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f764ccb0-0b2a-435c-9542-bdbdb06a3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相当于将所有的 data 从 pikle 中加载出来然后全部拼接起来\n",
    "for ds_file in data_sets_files:\n",
    "    #dataset = dataset.append(load(data_sets_dir, ds_file))\n",
    "    # 使用 pd.concat 来连接两个 DataFrame\n",
    "    dataset = pd.concat([dataset, load(data_sets_dir, ds_file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e79515a0-9813-473e-9f55-6528b41b0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集中的数据数量： 994\n"
     ]
    }
   ],
   "source": [
    "num_data = len(dataset)\n",
    "print(\"数据集中的数据数量：\", num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "977913c7-df82-4062-8587-f6178699aa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01,  4.6916e-04,  1.431...</td>\n",
       "      <td>0</td>\n",
       "      <td>[static, void, FUN1, (, void, *, VAR1, ,, uint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01,  2.6397e-04, -1.506...</td>\n",
       "      <td>0</td>\n",
       "      <td>[int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01, -1.0787e-03, -6.084...</td>\n",
       "      <td>1</td>\n",
       "      <td>[static, int, FUN1, (, AVFormatContext, *, VAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01,  2.5857e-03,  7.315...</td>\n",
       "      <td>1</td>\n",
       "      <td>[void, FUN1, (, AVDictionary, *, VAR1, ), {, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01,  1.5140e-03, -9.458...</td>\n",
       "      <td>0</td>\n",
       "      <td>[av_cold, void, FUN1, (, AVCodecContext, *, VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01, -1.3791e-03, -3.776...</td>\n",
       "      <td>1</td>\n",
       "      <td>[int, FUN1, (, AVIOContext, *, VAR1, ,, unsign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6774</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01, -2.2478e-03, -1.673...</td>\n",
       "      <td>1</td>\n",
       "      <td>[static, int, FUN1, (, struct, playlist, *, VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01,  1.4559e-03, -7.945...</td>\n",
       "      <td>1</td>\n",
       "      <td>[void, FUN1, (, unsigned, VAR1, ,, unsigned, *...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01, -3.5779e-04,  3.149...</td>\n",
       "      <td>1</td>\n",
       "      <td>[static, void, FUN1, (, CinepakEncContext, *, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>[(x, [tensor([ 1.5000e+01, -1.6496e-03,  7.380...</td>\n",
       "      <td>1</td>\n",
       "      <td>[static, uint32_t, FUN1, (, RiceContext, *, VA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  target  \\\n",
       "2     [(x, [tensor([ 1.5000e+01,  4.6916e-04,  1.431...       0   \n",
       "4     [(x, [tensor([ 1.5000e+01,  2.6397e-04, -1.506...       0   \n",
       "5     [(x, [tensor([ 1.5000e+01, -1.0787e-03, -6.084...       1   \n",
       "11    [(x, [tensor([ 1.5000e+01,  2.5857e-03,  7.315...       1   \n",
       "19    [(x, [tensor([ 1.5000e+01,  1.5140e-03, -9.458...       0   \n",
       "...                                                 ...     ...   \n",
       "6773  [(x, [tensor([ 1.5000e+01, -1.3791e-03, -3.776...       1   \n",
       "6774  [(x, [tensor([ 1.5000e+01, -2.2478e-03, -1.673...       1   \n",
       "6776  [(x, [tensor([ 1.5000e+01,  1.4559e-03, -7.945...       1   \n",
       "6786  [(x, [tensor([ 1.5000e+01, -3.5779e-04,  3.149...       1   \n",
       "6789  [(x, [tensor([ 1.5000e+01, -1.6496e-03,  7.380...       1   \n",
       "\n",
       "                                                   func  \n",
       "2     [static, void, FUN1, (, void, *, VAR1, ,, uint...  \n",
       "4     [int, FUN1, (, cl_mem, VAR1, ,, uint8_t, *, VA...  \n",
       "5     [static, int, FUN1, (, AVFormatContext, *, VAR...  \n",
       "11    [void, FUN1, (, AVDictionary, *, VAR1, ), {, A...  \n",
       "19    [av_cold, void, FUN1, (, AVCodecContext, *, VA...  \n",
       "...                                                 ...  \n",
       "6773  [int, FUN1, (, AVIOContext, *, VAR1, ,, unsign...  \n",
       "6774  [static, int, FUN1, (, struct, playlist, *, VA...  \n",
       "6776  [void, FUN1, (, unsigned, VAR1, ,, unsigned, *...  \n",
       "6786  [static, void, FUN1, (, CinepakEncContext, *, ...  \n",
       "6789  [static, uint32_t, FUN1, (, RiceContext, *, VA...  \n",
       "\n",
       "[994 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "022b90f0-46e7-4e9d-9376-fbcc84d4304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb40116c-308d-4cf9-86af-97ad5becf181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Dataset\n"
     ]
    }
   ],
   "source": [
    "# split the dataset and pass to DataLoader with batch size\n",
    "\n",
    "train_loader, val_loader, test_loader = list(\n",
    "        map(lambda x: x.get_loader(context.batch_size, shuffle=context.shuffle),\n",
    "            train_val_test_split(dataset, shuffle=context.shuffle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c503776-b270-4f2d-8ab6-e344b1adfa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x7f75b27f5dc0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d80fed7-c285-40b5-b711-4d2bff6fe478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1640, 101], edge_index=[2, 952], y=[8], func=[8], batch=[1640], ptr=[9])\n"
     ]
    }
   ],
   "source": [
    "# x=[1640, 101]: x 是节点特征矩阵，形状为 [1640, 101]。其中，1640 是批次中的节点数量，101 是每个节点的特征维度。\n",
    "# edge_index=[2, 933]: edge_index 是边索引矩阵，形状为 [2, 933]。其中，2 表示每列有两个元素，表示一条边的起始节点和目标节点的索引。933 是批次中的边数量。\n",
    "# y=[8]: y 是标签，形状为 [8]。这里的 8 可能表示批次中的样本数量。\n",
    "# batch=[1640]: batch 是用于指示节点属于哪个图批次的索引数组，形状为 [1640]。这里的 1640 可能表示批次中的节点数量。\n",
    "# ptr=[9]: ptr 是一个指针数组，指示批次中每个图的节点范围，形状为 [9]。这里的 9 可能表示批次中的图数量\n",
    "\n",
    "\n",
    "batch = next(iter(train_loader))  # 获取第一个批次\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a1eddf5-ac9d-46bc-ac76-91587ad5c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "false = dataset[dataset.target == 0]\n",
    "true = dataset[dataset.target == 1]\n",
    "shuffle = True\n",
    "\n",
    "train_false, test_false = train_test_split(false, test_size=0.2, shuffle=shuffle)\n",
    "test_false, val_false = train_test_split(test_false, test_size=0.5, shuffle=shuffle)\n",
    "train_true, test_true = train_test_split(true, test_size=0.2, shuffle=shuffle)\n",
    "test_true, val_true = train_test_split(test_true, test_size=0.5, shuffle=shuffle)\n",
    "\n",
    "# train = train_false.append(train_true)\n",
    "train = pd.concat([train_false, train_true])\n",
    "\n",
    "# val = val_false.append(val_true)\n",
    "val = pd.concat([val_false, val_true])\n",
    "\n",
    "# test = test_false.append(test_true)\n",
    "test =pd.concat([test_false, test_true])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438c494-2275-4b47-93d5-a6ada4caac07",
   "metadata": {},
   "source": [
    "### reset index and pack the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0db8139-4613-4bbf-87a8-9a7b2064508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.9931e-03,  9.583...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, struct, histogram_s, *, VAR1, ,, flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01, -2.6061e-03,  1.380...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, int, VAR1, ,, int, VAR2, ,, int, *, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.6988e-03,  9.591...</td>\n",
       "      <td>0</td>\n",
       "      <td>[VAR1, (, JNIEnv, *, VAR2, ,, jclass, VAR3, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.6988e-03,  9.591...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, const, libcec_alert, VAR1, ,, const,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.8835e-04,  1.114...</td>\n",
       "      <td>0</td>\n",
       "      <td>[FUN1, (, int, VAR1, ,, Datum, VAR2, ), {, FIL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  2.9198e-03,  4.542...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, char, *, VAR1, ,, int, class, ), {, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.4122e-03,  1.009...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, Btree, *, VAR1, ,, MemPage, *, VAR2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.8965e-03,  9.435...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, cpl_table, *, VAR1, ,, const, cpl_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.9931e-03,  9.583...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, int, VAR1, ,, char, *, VAR2, ), {, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>[(x, [tensor([ 6.6000e+01,  1.4122e-03,  1.009...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FUN1, (, struct, _WapiFileShare, *, VAR1, ,, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  target  \\\n",
       "0    [(x, [tensor([ 6.6000e+01,  1.9931e-03,  9.583...       0   \n",
       "1    [(x, [tensor([ 6.6000e+01, -2.6061e-03,  1.380...       0   \n",
       "2    [(x, [tensor([ 6.6000e+01,  1.6988e-03,  9.591...       0   \n",
       "3    [(x, [tensor([ 6.6000e+01,  1.6988e-03,  9.591...       0   \n",
       "4    [(x, [tensor([ 6.6000e+01,  2.8835e-04,  1.114...       0   \n",
       "..                                                 ...     ...   \n",
       "826  [(x, [tensor([ 6.6000e+01,  2.9198e-03,  4.542...       1   \n",
       "827  [(x, [tensor([ 6.6000e+01,  1.4122e-03,  1.009...       1   \n",
       "828  [(x, [tensor([ 6.6000e+01,  1.8965e-03,  9.435...       1   \n",
       "829  [(x, [tensor([ 6.6000e+01,  1.9931e-03,  9.583...       1   \n",
       "830  [(x, [tensor([ 6.6000e+01,  1.4122e-03,  1.009...       1   \n",
       "\n",
       "                                                  func  \n",
       "0    [FUN1, (, struct, histogram_s, *, VAR1, ,, flo...  \n",
       "1    [FUN1, (, int, VAR1, ,, int, VAR2, ,, int, *, ...  \n",
       "2    [VAR1, (, JNIEnv, *, VAR2, ,, jclass, VAR3, ,,...  \n",
       "3    [FUN1, (, const, libcec_alert, VAR1, ,, const,...  \n",
       "4    [FUN1, (, int, VAR1, ,, Datum, VAR2, ), {, FIL...  \n",
       "..                                                 ...  \n",
       "826  [FUN1, (, char, *, VAR1, ,, int, class, ), {, ...  \n",
       "827  [FUN1, (, Btree, *, VAR1, ,, MemPage, *, VAR2,...  \n",
       "828  [FUN1, (, cpl_table, *, VAR1, ,, const, cpl_pr...  \n",
       "829  [FUN1, (, int, VAR1, ,, char, *, VAR2, ), {, V...  \n",
       "830  [FUN1, (, struct, _WapiFileShare, *, VAR1, ,, ...  \n",
       "\n",
       "[831 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0af74ef3-50b8-48de-8186-49e70aaed2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputDataset(TorchDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return self.dataset.iloc[index].input\n",
    "        data = self.dataset.iloc[index].input\n",
    "        data.func = self.dataset.iloc[index].func  # 添加现有的func属性, 为 codebert 的输入准备\n",
    "        return data\n",
    "\n",
    "    def get_loader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(dataset=self, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82c89855-cf00-4c75-b91d-cfe551b6572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Dataset\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = list(\n",
    "        map(lambda x: x.get_loader(context.batch_size, shuffle=context.shuffle),\n",
    "            train_val_test_split(dataset, shuffle=context.shuffle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3c0beb7-b525-4db1-9d29-a712b7004cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "DataBatch(x=[1640, 101], edge_index=[2, 933], y=[8], func=[8], batch=[1640], ptr=[9])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(\"Batch\", i)\n",
    "    print(batch)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0853bcd8-ff41-4d32-9be6-4ab250533b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[1640, 101], edge_index=[2, 933], y=[8], func=[8], batch=[1640], ptr=[9])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "940dcaf3-ffd2-4704-9842-3c4232868fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index, func = batch.x, batch.edge_index, batch.func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4895a76d-b680-4086-93d3-c284cbecdf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUN1(intVAR1,intVAR2,intVAR3,intVAR4){intVAR5;FUN2(,VAR1,VAR2,VAR3,VAR4);while(VAR4>0){VAR5=VAR3;while(VAR5>0){FUN3(VAR2+VAR4-2,VAR1+VAR5-2,);VAR5--;}VAR4--;}}\n"
     ]
    }
   ],
   "source": [
    "func[0]\n",
    "result = ''.join(func[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a053f87-88fe-4fbc-ae93-00aafe6e85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for var in func:\n",
    "    result = ''.join(var)\n",
    "    text.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4b76ae0-2f8f-4b3a-adfd-5c066f3e3d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7876, 0.2214],\n",
       "        [0.7635, 0.2402],\n",
       "        [0.8344, 0.1882],\n",
       "        [0.6971, 0.3014],\n",
       "        [0.6575, 0.2543],\n",
       "        [0.7082, 0.3818],\n",
       "        [0.7562, 0.2437],\n",
       "        [0.7343, 0.3045]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(input_ids.to(\"cuda\"), attention_mask.to(\"cuda\"))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1151f455-1a3e-47d8-9966-08de4b9f03be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6379, 0.3621],\n",
       "        [0.6279, 0.3721],\n",
       "        [0.6562, 0.3438],\n",
       "        [0.5977, 0.4023],\n",
       "        [0.5994, 0.4006],\n",
       "        [0.5809, 0.4191],\n",
       "        [0.6254, 0.3746],\n",
       "        [0.6058, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.nn.Softmax(dim=1)(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e6ad2c0-0969-4bf6-87cd-fa2fc34e1c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6873, 0.5551],\n",
       "        [0.6821, 0.5598],\n",
       "        [0.6973, 0.5469],\n",
       "        [0.6676, 0.5748],\n",
       "        [0.6587, 0.5632],\n",
       "        [0.6700, 0.5943],\n",
       "        [0.6805, 0.5606],\n",
       "        [0.6757, 0.5755]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51a3199a-dc77-407b-83d7-408ffef6d34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01c91c77-9e41-4688-815a-48ddb581a33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
